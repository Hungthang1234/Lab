{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOKhE71L9BC2NwcUDCUTHD2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"izIOwqZv7BgO"},"outputs":[],"source":["# C√†i Java\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","\n","# T·∫£i v√† gi·∫£i n√©n Spark\n","!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n","!tar xf spark-3.5.1-bin-hadoop3.tgz\n","\n","# C√†i findspark ƒë·ªÉ li√™n k·∫øt Spark v·ªõi Colab\n","!pip install -q findspark\n"]},{"cell_type":"markdown","source":["cai da spark\n"],"metadata":{"id":"ygeoHNq87pwe"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"HousePricePrediction\").getOrCreate()\n"],"metadata":{"id":"nCdNIIlS_hgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    from pyspark.sql import SparkSession\n","\n","    spark = SparkSession.builder.appName(\"CheckSpark\").getOrCreate()\n","    print(\"‚úÖ Spark ƒë√£ ƒë∆∞·ª£c c√†i!\")\n","    print(\"üîç Phi√™n b·∫£n Spark:\", spark.version)\n","except Exception as e:\n","    print(\"‚ùå Spark ch∆∞a ƒë∆∞·ª£c c√†i.\")\n","    print(\"L·ªói:\", e)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vXDgn3J7kQU","executionInfo":{"status":"ok","timestamp":1750516805915,"user_tz":-420,"elapsed":6422,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"60506810-270e-4d5d-8d9a-5ef77811ebac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Spark ƒë√£ ƒë∆∞·ª£c c√†i!\n","üîç Phi√™n b·∫£n Spark: 3.5.1\n"]}]},{"cell_type":"code","source":["!java -version\n","!readlink -f $(which java)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kgT5a5p889r","executionInfo":{"status":"ok","timestamp":1750517158613,"user_tz":-420,"elapsed":541,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"337d44d0-e545-45d8-8117-ab63898fbf16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["openjdk version \"11.0.27\" 2025-04-15\n","OpenJDK Runtime Environment (build 11.0.27+6-post-Ubuntu-0ubuntu122.04)\n","OpenJDK 64-Bit Server VM (build 11.0.27+6-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n","/usr/lib/jvm/java-11-openjdk-amd64/bin/java\n"]}]},{"cell_type":"code","source":["try:\n","    import pyspark\n","    print(\"‚úÖ PySpark ƒë√£ ƒë∆∞·ª£c c√†i.\")\n","except ImportError:\n","    print(\"‚ùå PySpark ch∆∞a ƒë∆∞·ª£c c√†i.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jb5YtHJA9Gy7","executionInfo":{"status":"ok","timestamp":1750599549572,"user_tz":-420,"elapsed":435,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"243c4c7f-0ae5-4f37-a5af-973bf5f7a30a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ PySpark ƒë√£ ƒë∆∞·ª£c c√†i.\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.regression import LinearRegression\n","\n","spark = SparkSession.builder \\\n","    .appName(\"MLlib Test\") \\\n","    .getOrCreate()\n","\n","print(spark.version)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q96wq0lY9fz2","executionInfo":{"status":"ok","timestamp":1750517286595,"user_tz":-420,"elapsed":216,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"8af54527-92b7-4784-854a-b5460a88c9de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.5.1\n"]}]},{"cell_type":"markdown","source":["check\n"],"metadata":{"id":"kwyCtUUv7r3C"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"HousePricePrediction\") \\\n","    .getOrCreate()\n"],"metadata":{"id":"3c__c5nU7uGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n","lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n"],"metadata":{"id":"N07IVE_d9Omh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"MLlibTest\").getOrCreate()\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"wO6XGNmy9Was","executionInfo":{"status":"ok","timestamp":1750517248186,"user_tz":-420,"elapsed":289,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"6ca6ea92-d037-41a9-b05f-b14efed1e2f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7c7b877a2890>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://357c9aacbe54:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>CheckSpark</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["import spark"],"metadata":{"id":"i55Opjz68HsD"}},{"cell_type":"code","source":["import os\n","import findspark\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n","\n","findspark.init()\n","\n","\n"],"metadata":{"id":"RP-DyZAt7zgs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cai dat moi truong"],"metadata":{"id":"vULfZT2Y8hVT"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fi3OUxWh8z-i","executionInfo":{"status":"ok","timestamp":1750517440017,"user_tz":-420,"elapsed":41062,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"57000305-2017-4468-df00-b7efb9494af5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["csv_path = \"/content/drive/MyDrive/Big Data/Project/data/HousePriceTraindatatest.csv\"\n"],"metadata":{"id":"BuvbEQIF-JhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = spark.read.csv(csv_path, header=True, inferSchema=True)\n","data.show(5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L1rgN37f-YcP","executionInfo":{"status":"ok","timestamp":1750517526587,"user_tz":-420,"elapsed":9310,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"7dbc2779-bc91-48ab-8ee1-3eebee96d3dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+---------+--------+---------+-----------+--------+------+----------+----+---------+----------+-------------+--------+------------+--------------------+---------+--------+-------+\n","|               date|    price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|sqft_above|sqft_basement|yr_built|yr_renovated|              street|     city|statezip|country|\n","+-------------------+---------+--------+---------+-----------+--------+------+----------+----+---------+----------+-------------+--------+------------+--------------------+---------+--------+-------+\n","|2014-05-02 00:00:00| 313000.0|     3.0|      1.5|       1340|    7912|   1.5|         0|   0|        3|      1340|            0|    1955|        2005|18810 Densmore Ave N|Shoreline|WA 98133|    USA|\n","|2014-05-02 00:00:00|2384000.0|     5.0|      2.5|       3650|    9050|   2.0|         0|   4|        5|      3370|          280|    1921|           0|     709 W Blaine St|  Seattle|WA 98119|    USA|\n","|2014-05-02 00:00:00| 342000.0|     3.0|      2.0|       1930|   11947|   1.0|         0|   0|        4|      1930|            0|    1966|           0|26206-26214 143rd...|     Kent|WA 98042|    USA|\n","|2014-05-02 00:00:00| 420000.0|     3.0|     2.25|       2000|    8030|   1.0|         0|   0|        4|      1000|         1000|    1963|           0|     857 170th Pl NE| Bellevue|WA 98008|    USA|\n","|2014-05-02 00:00:00| 550000.0|     4.0|      2.5|       1940|   10500|   1.0|         0|   0|        4|      1140|          800|    1976|        1992|   9105 170th Ave NE|  Redmond|WA 98052|    USA|\n","+-------------------+---------+--------+---------+-----------+--------+------+----------+----+---------+----------+-------------+--------+------------+--------------------+---------+--------+-------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["data.printSchema()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxWUwqqA-lfc","executionInfo":{"status":"ok","timestamp":1750517661256,"user_tz":-420,"elapsed":153,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"ba657181-268a-4d9b-bf07-3e6af0872f1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- date: timestamp (nullable = true)\n"," |-- price: double (nullable = true)\n"," |-- bedrooms: double (nullable = true)\n"," |-- bathrooms: double (nullable = true)\n"," |-- sqft_living: integer (nullable = true)\n"," |-- sqft_lot: integer (nullable = true)\n"," |-- floors: double (nullable = true)\n"," |-- waterfront: integer (nullable = true)\n"," |-- view: integer (nullable = true)\n"," |-- condition: integer (nullable = true)\n"," |-- sqft_above: integer (nullable = true)\n"," |-- sqft_basement: integer (nullable = true)\n"," |-- yr_built: integer (nullable = true)\n"," |-- yr_renovated: integer (nullable = true)\n"," |-- street: string (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- statezip: string (nullable = true)\n"," |-- country: string (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt\n","cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","        'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built',\n","        'yr_renovated', 'price']\n","\n","# √âp ki·ªÉu d·ªØ li·ªáu sang s·ªë th·ª±c\n","data = data.select([col(c).cast(\"double\") for c in cols])\n","\n","# Xo√° c√°c d√≤ng ch·ª©a gi√° tr·ªã null\n","data = data.dropna()\n"],"metadata":{"id":"xbyvrCul_6j9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","\n","feature_cols = [c for c in data.columns if c != 'price']\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n","\n","data = assembler.transform(data).select('features', 'price')\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n"],"metadata":{"id":"duDYyRBD_7mM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n","\n","# T·∫°o m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh\n","lr = LinearRegression(featuresCol='features', labelCol='price')\n","\n","# Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi t·∫≠p train\n","lr_model = lr.fit(train_data)\n"],"metadata":{"id":"DShLPCzqABsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# D·ª± ƒëo√°n tr√™n t·∫≠p test\n","predictions = lr_model.transform(test_data)\n","predictions.select(\"price\", \"prediction\").show(5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SgBHZ3cJADIv","executionInfo":{"status":"ok","timestamp":1750517959077,"user_tz":-420,"elapsed":1708,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"6bc6618e-62c0-4e67-eaee-63af086441ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+------------------+\n","|   price|        prediction|\n","+--------+------------------+\n","|276000.0| 288541.8034555251|\n","|299000.0|181040.08187181223|\n","|190000.0|328128.92167808395|\n","|129000.0| 236825.0029925732|\n","|     0.0|307894.53907257505|\n","+--------+------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# T·∫°o b·ªô ƒë√°nh gi√°\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n","\n","# T√≠nh to√°n c√°c ch·ªâ s·ªë\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","# In k·∫øt qu·∫£\n","print(f\"‚úÖ RMSE: {rmse:.2f}\")\n","print(f\"‚úÖ MAE: {mae:.2f}\")\n","print(f\"‚úÖ R2: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xz4yFZi8AFiY","executionInfo":{"status":"ok","timestamp":1750517977377,"user_tz":-420,"elapsed":1392,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"f7ab0b78-e624-4590-a839-d525e800d978"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ RMSE: 229256.84\n","‚úÖ MAE: 155462.02\n","‚úÖ R2: 0.5057\n"]}]},{"cell_type":"code","source":["# Hi·ªÉn th·ªã h·ªá s·ªë v√† ch·ªách\n","print(\"H·ªá s·ªë (coefficients):\", lr_model.coefficients)\n","print(\"Ch·ªách (intercept):\", lr_model.intercept)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nw0wPpy8AODM","executionInfo":{"status":"ok","timestamp":1750517999867,"user_tz":-420,"elapsed":120,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"629d5247-12cc-47ff-a655-ead6e0a0da32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["H·ªá s·ªë (coefficients): [-62942.235911520766,67394.52486084067,-2985.1855993470094,-0.70507718773557,24414.38452868245,54805.514202636456,30384.770278530894,3251.0768191587,3219.8779734690847,-2342.437296582884,6.509905399810215]\n","Ch·ªách (intercept): 4531996.92381375\n"]}]},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\n","\n","# T·∫°o m√¥ h√¨nh Random Forest\n","rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"price\", numTrees=100, maxDepth=10, seed=42)\n","\n","# Hu·∫•n luy·ªán m√¥ h√¨nh\n","rf_model = rf.fit(train_data)\n"],"metadata":{"id":"oN5mCHTUAUEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# D·ª± ƒëo√°n tr√™n t·∫≠p test\n","rf_predictions = rf_model.transform(test_data)\n","rf_predictions.select(\"price\", \"prediction\").show(5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xmqn8UD7AXFd","executionInfo":{"status":"ok","timestamp":1750518044774,"user_tz":-420,"elapsed":1786,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"60471f5a-6f0d-4e18-a8a0-86da423fe82e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------+------------------+\n","|   price|        prediction|\n","+--------+------------------+\n","|276000.0| 278359.6637395524|\n","|299000.0|266920.43134508276|\n","|190000.0| 304434.8979603931|\n","|129000.0|261578.90851653053|\n","|     0.0| 274759.8880366312|\n","+--------+------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# T·∫°o b·ªô ƒë√°nh gi√°\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n","\n","# T√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°\n","rf_rmse = evaluator.setMetricName(\"rmse\").evaluate(rf_predictions)\n","rf_mae = evaluator.setMetricName(\"mae\").evaluate(rf_predictions)\n","rf_r2 = evaluator.setMetricName(\"r2\").evaluate(rf_predictions)\n","\n","# In k·∫øt qu·∫£\n","print(f\"üå≤ Random Forest RMSE: {rf_rmse:.2f}\")\n","print(f\"üå≤ Random Forest MAE: {rf_mae:.2f}\")\n","print(f\"üå≤ Random Forest R2: {rf_r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8mh5avLAYkv","executionInfo":{"status":"ok","timestamp":1750518050628,"user_tz":-420,"elapsed":2399,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"f5ee00f7-04f1-4557-fee2-a01e4f65f317"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üå≤ Random Forest RMSE: 260588.23\n","üå≤ Random Forest MAE: 165450.60\n","üå≤ Random Forest R2: 0.3614\n"]}]},{"cell_type":"code","source":["# ƒê·ªçc l·∫°i d·ªØ li·ªáu t·ª´ file CSV g·ªëc\n","data = spark.read.csv(csv_path, header=True, inferSchema=True)\n"],"metadata":{"id":"tb8zfPVnAnVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","cat_cols = ['city', 'statezip']\n","num_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","            'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built',\n","            'yr_renovated']\n","label_col = 'price'\n","\n","# L·∫•y ƒë√∫ng c·ªôt v√† √©p ki·ªÉu s·ªë cho numeric\n","data = data.select(\n","    [col(c).cast(\"double\") if c in num_cols + [label_col] else col(c) for c in num_cols + cat_cols + [label_col]]\n",").dropna()\n"],"metadata":{"id":"vEiKhOPQAhy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n","\n","indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\") for c in cat_cols]\n"],"metadata":{"id":"zuiHKIJdBBIz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.regression import RandomForestRegressor\n","\n","# G·ªôp c·ªôt ƒë·∫∑c tr∆∞ng\n","feature_cols = num_cols + [c + \"_idx\" for c in cat_cols]\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","# M√¥ h√¨nh Random Forest ƒë√£ ch·ªânh maxBins\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"price\",\n","    numTrees=100,\n","    maxDepth=10,\n","    maxBins=128,\n","    seed=42\n",")\n","\n","# Pipeline g·ªìm: m√£ h√≥a + vector h√≥a + m√¥ h√¨nh\n","pipeline = Pipeline(stages=indexers + [assembler, rf])\n","\n","# T√°ch t·∫≠p train/test\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Hu·∫•n luy·ªán m√¥ h√¨nh\n","rf_model = pipeline.fit(train_data)\n","\n","# D·ª± ƒëo√°n\n","predictions = rf_model.transform(test_data)\n"],"metadata":{"id":"goHA8cOzBGkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n","\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","print(f\"üè° Random Forest + City/Zip RMSE: {rmse:.2f}\")\n","print(f\"üè° Random Forest + City/Zip MAE: {mae:.2f}\")\n","print(f\"üè° Random Forest + City/Zip R2: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zw-HR19kBkya","executionInfo":{"status":"ok","timestamp":1750518356226,"user_tz":-420,"elapsed":1549,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"877f04ee-715d-4ef4-db79-bc331966ca52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üè° Random Forest + City/Zip RMSE: 232481.79\n","üè° Random Forest + City/Zip MAE: 120134.04\n","üè° Random Forest + City/Zip R2: 0.4917\n"]}]},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# T·∫°o l·∫°i indexer v√† assembler\n","indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\") for c in ['city', 'statezip']]\n","feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","                'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built',\n","                'yr_renovated', 'city_idx', 'statezip_idx']\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","# B·ªô ƒë√°nh gi√°\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n","\n","# Danh s√°ch m√¥ h√¨nh c·∫ßn th·ª≠\n","param_sets = [\n","    {\"numTrees\": 100, \"maxDepth\": 10, \"maxBins\": 128},\n","    {\"numTrees\": 200, \"maxDepth\": 15, \"maxBins\": 128},\n","    {\"numTrees\": 300, \"maxDepth\": 20, \"maxBins\": 256},\n","]\n","\n","for i, params in enumerate(param_sets, start=1):\n","    print(f\"\\nüîç ƒêang ch·∫°y m√¥ h√¨nh c·∫•u h√¨nh {i}: {params}\")\n","\n","    rf = RandomForestRegressor(\n","        featuresCol=\"features\",\n","        labelCol=\"price\",\n","        numTrees=params[\"numTrees\"],\n","        maxDepth=params[\"maxDepth\"],\n","        maxBins=params[\"maxBins\"],\n","        seed=42\n","    )\n","\n","    pipeline = Pipeline(stages=indexers + [assembler, rf])\n","    model = pipeline.fit(train_data)\n","    predictions = model.transform(test_data)\n","\n","    rmse = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(predictions)\n","    mae = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(predictions)\n","    r2 = evaluator.evaluate(predictions)\n","\n","    print(f\"üß™ Config {i} ‚û§ RMSE: {rmse:.2f}, MAE: {mae:.2f}, R¬≤: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KwnhfxcLByg4","executionInfo":{"status":"ok","timestamp":1750519232445,"user_tz":-420,"elapsed":821274,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"4fbf9274-3262-4add-d417-cceb1e281891"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîç ƒêang ch·∫°y m√¥ h√¨nh c·∫•u h√¨nh 1: {'numTrees': 100, 'maxDepth': 10, 'maxBins': 128}\n","üß™ Config 1 ‚û§ RMSE: 232481.79, MAE: 120134.04, R¬≤: 0.4917\n","\n","üîç ƒêang ch·∫°y m√¥ h√¨nh c·∫•u h√¨nh 2: {'numTrees': 200, 'maxDepth': 15, 'maxBins': 128}\n","üß™ Config 2 ‚û§ RMSE: 230352.25, MAE: 119988.40, R¬≤: 0.5009\n","\n","üîç ƒêang ch·∫°y m√¥ h√¨nh c·∫•u h√¨nh 3: {'numTrees': 300, 'maxDepth': 20, 'maxBins': 256}\n","üß™ Config 3 ‚û§ RMSE: 222681.04, MAE: 118275.83, R¬≤: 0.5336\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col, when, year, to_date\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.regression import GBTRegressor\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# B∆∞·ªõc 1: ƒê·ªçc l·∫°i d·ªØ li·ªáu g·ªëc\n","data = spark.read.csv(csv_path, header=True, inferSchema=True)\n","\n","# B∆∞·ªõc 2: T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi\n","data = data.withColumn(\"age_of_house\", 2025 - col(\"yr_built\"))\n","data = data.withColumn(\"was_renovated\", when(col(\"yr_renovated\") > 0, 1).otherwise(0))\n","data = data.withColumn(\"total_area\", col(\"sqft_living\") + col(\"sqft_basement\"))\n","\n","# √âp ki·ªÉu c√°c c·ªôt s·ªë\n","numeric_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","                'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built',\n","                'yr_renovated', 'age_of_house', 'was_renovated', 'total_area', 'price']\n","\n","for c in numeric_cols:\n","    data = data.withColumn(c, col(c).cast(\"double\"))\n","\n","# B∆∞·ªõc 3: M√£ h√≥a bi·∫øn ph√¢n lo·∫°i\n","cat_cols = ['city', 'statezip']\n","indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\") for c in cat_cols]\n","\n","# T·∫°o danh s√°ch c·ªôt ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o\n","feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","                'view', 'condition', 'sqft_above', 'sqft_basement', 'age_of_house',\n","                'was_renovated', 'total_area', 'city_idx', 'statezip_idx']\n","\n","# VectorAssembler\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","# B∆∞·ªõc 4: T·∫°o m√¥ h√¨nh GBT\n","gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"price\", maxIter=200, maxDepth=7, maxBins=256)\n","\n","# T·∫°o pipeline t·ªïng th·ªÉ\n","pipeline = Pipeline(stages=indexers + [assembler, gbt])\n","\n","# B∆∞·ªõc 5: X·ª≠ l√Ω thi·∫øu d·ªØ li·ªáu v√† chia train/test\n","data = data.dropna()\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Hu·∫•n luy·ªán m√¥ h√¨nh\n","gbt_model = pipeline.fit(train_data)\n","\n","# D·ª± ƒëo√°n\n","predictions = gbt_model.transform(test_data)\n"],"metadata":{"id":"SGNBRZJTEUz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tr∆∞·ªõc ƒë√¢y (l·ªói):\n","# indexers = [StringIndexer(inputCol=c, outputCol=c + \"_idx\") for c in ['city', 'statezip']]\n","\n","# S·ª≠a l·∫°i:\n","indexers = [\n","    StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\")\n","    for c in ['city', 'statezip']\n","]\n"],"metadata":{"id":"InooP0PTHQRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = spark.read.csv(csv_path, header=True, inferSchema=True)\n"],"metadata":{"id":"6JICcNiJFhny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, when\n","\n","data = data.withColumn(\"age_of_house\", 2025 - col(\"yr_built\"))\n","data = data.withColumn(\"was_renovated\", when(col(\"yr_renovated\") > 0, 1).otherwise(0))\n","data = data.withColumn(\"total_area\", col(\"sqft_living\") + col(\"sqft_basement\"))\n"],"metadata":{"id":"1eJkf7KgHfWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numeric_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","                'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built',\n","                'yr_renovated', 'age_of_house', 'was_renovated', 'total_area', 'price']\n","\n","for c in numeric_cols:\n","    data = data.withColumn(c, col(c).cast(\"double\"))\n","\n","data = data.dropna()\n"],"metadata":{"id":"2eUMeY6yHhU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n","\n","cat_cols = ['city', 'statezip']\n","indexers = [\n","    StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\")\n","    for c in cat_cols\n","]\n"],"metadata":{"id":"mo_WIUjFHjwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.regression import GBTRegressor\n","from pyspark.ml import Pipeline\n","\n","feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","                'view', 'condition', 'sqft_above', 'sqft_basement', 'age_of_house',\n","                'was_renovated', 'total_area', 'city_idx', 'statezip_idx']\n","\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"price\", maxIter=200, maxDepth=7, maxBins=256)\n","\n","pipeline = Pipeline(stages=indexers + [assembler, gbt])\n"],"metadata":{"id":"0DyNYG6tHkmt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","gbt_model = pipeline.fit(train_data)\n","predictions = gbt_model.transform(test_data)\n"],"metadata":{"id":"1YbBLsDjHmXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","print(f\"GBTRegressor RMSE: {rmse:.2f}\")\n","print(f\"GBTRegressor MAE: {mae:.2f}\")\n","print(f\"GBTRegressor R¬≤: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fgTcVmbHoFM","executionInfo":{"status":"ok","timestamp":1750520345791,"user_tz":-420,"elapsed":1971,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"3172f2ee-cefe-4bbb-b72e-70684daf2a95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GBTRegressor RMSE: 1036064.87\n","GBTRegressor MAE: 175768.25\n","GBTRegressor R¬≤: 0.0231\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import log1p\n","\n","# T·∫°o bi·∫øn m·ªõi log_price = log(1 + price)\n","data = data.withColumn(\"log_price\", log1p(col(\"price\")))\n"],"metadata":{"id":"P2Ma4SDDJWhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"log_price\", maxIter=200, maxDepth=7, maxBins=256)\n"],"metadata":{"id":"OysK8zdtJYIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import expm1\n","\n","# D·ª± ƒëo√°n xong r·ªìi:\n","predictions = gbt_model.transform(test_data)\n","\n","# Chuy·ªÉn t·ª´ log(price) v·ªÅ l·∫°i price\n","predictions = predictions.withColumn(\"prediction_price\", expm1(col(\"prediction\")))\n"],"metadata":{"id":"lhV98eujJZeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction_price\")\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","print(f\" GBT (log-price) RMSE: {rmse:.2f}\")\n","print(f\" GBT (log-price) MAE: {mae:.2f}\")\n","print(f\" GBT (log-price) R¬≤: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqliJ8fBJbqt","executionInfo":{"status":"ok","timestamp":1750520486444,"user_tz":-420,"elapsed":2062,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"a52f6943-4307-47cb-f7ba-865720576830"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" GBT (log-price) RMSE: inf\n"," GBT (log-price) MAE: inf\n"," GBT (log-price) R¬≤: -inf\n"]}]},{"cell_type":"code","source":["# Ch·ªâ gi·ªØ c√°c d√≤ng c√≥ price > 0\n","data = data.filter(col(\"price\") > 0)\n","\n","# T·∫°o log_price sau khi l·ªçc\n","from pyspark.sql.functions import log1p\n","data = data.withColumn(\"log_price\", log1p(col(\"price\")))\n"],"metadata":{"id":"Ynv5MWg5Jh56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import expm1\n","\n","predictions = gbt_model.transform(test_data)\n","predictions = predictions.withColumn(\"prediction_price\", expm1(col(\"prediction\")))\n"],"metadata":{"id":"ziETjhjMJill"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction_price\")\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","print(f\"‚úÖ GBT (log-price, clean) RMSE: {rmse:.2f}\")\n","print(f\"‚úÖ GBT (log-price, clean) MAE: {mae:.2f}\")\n","print(f\"‚úÖ GBT (log-price, clean) R¬≤: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4J-1wexJvq0","executionInfo":{"status":"ok","timestamp":1750520497039,"user_tz":-420,"elapsed":1021,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"5a5b55ff-d8de-4619-b912-51b4591a594c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ GBT (log-price, clean) RMSE: inf\n","‚úÖ GBT (log-price, clean) MAE: inf\n","‚úÖ GBT (log-price, clean) R¬≤: -inf\n"]}]},{"cell_type":"code","source":["predictions.select(\"prediction\", \"price\").show(10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4Y16ee_J0Uj","executionInfo":{"status":"ok","timestamp":1750520515493,"user_tz":-420,"elapsed":358,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"4a0bd287-e23e-4122-a7cd-afea65b36170"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+--------+\n","|        prediction|   price|\n","+------------------+--------+\n","|245468.95266441183|242500.0|\n","| 378104.7593569519|284000.0|\n","| 389074.3013421495|287200.0|\n","|324690.35468439764|315000.0|\n","| 433643.0973688922|382500.0|\n","|422664.36640199134|407500.0|\n","|502946.56333111867|437500.0|\n","|450136.51794225466|459990.0|\n","| 647791.0334011437|555000.0|\n","| 652015.5432094008|588500.0|\n","+------------------+--------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n","\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","print(f\"‚úÖ GBT (gi√° g·ªëc) RMSE: {rmse:.2f}\")\n","print(f\"‚úÖ GBT (gi√° g·ªëc) MAE: {mae:.2f}\")\n","print(f\"‚úÖ GBT (gi√° g·ªëc) R¬≤: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8T8IChpWJ8YF","executionInfo":{"status":"ok","timestamp":1750520549549,"user_tz":-420,"elapsed":1414,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"20ee7c70-d7f7-400c-cb90-a7bad0f0565d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ GBT (gi√° g·ªëc) RMSE: 1036064.87\n","‚úÖ GBT (gi√° g·ªëc) MAE: 175768.25\n","‚úÖ GBT (gi√° g·ªëc) R¬≤: 0.0231\n"]}]},{"cell_type":"code","source":["# Gi·∫£ s·ª≠ rf_model l√† m√¥ h√¨nh Random Forest t·ªët nh·∫•t ƒë√£ hu·∫•n luy·ªán (config 3)\n","rf_model.save(\"/content/best_rf_model\")\n"],"metadata":{"id":"OkEzXQVaKJKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# N√©n th∆∞ m·ª•c model\n","!zip -r best_rf_model.zip best_rf_model\n","\n","# T·∫£i file zip v·ªÅ\n","from google.colab import files\n","files.download(\"best_rf_model.zip\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":902},"id":"4a_y780lKMPl","executionInfo":{"status":"ok","timestamp":1750520613703,"user_tz":-420,"elapsed":579,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"8fbd473c-9699-4494-a18d-03d359e102b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: best_rf_model/ (stored 0%)\n","  adding: best_rf_model/stages/ (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/ (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/treesMetadata/ (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/treesMetadata/.part-00000-9b9e97b7-6524-4184-85b1-9c4a34a26c7c-c000.snappy.parquet.crc (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/treesMetadata/part-00000-9b9e97b7-6524-4184-85b1-9c4a34a26c7c-c000.snappy.parquet (deflated 25%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/treesMetadata/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/treesMetadata/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/data/ (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/data/part-00000-3eeb13e3-a986-4f4a-af2e-31c5ba82f05d-c000.snappy.parquet (deflated 11%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/data/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/data/.part-00000-3eeb13e3-a986-4f4a-af2e-31c5ba82f05d-c000.snappy.parquet.crc (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/data/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/metadata/ (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/metadata/.part-00000.crc (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/metadata/part-00000 (deflated 44%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/metadata/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/3_RandomForestRegressor_0238ed250c3a/metadata/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/stages/2_VectorAssembler_89c2eb4ae44a/ (stored 0%)\n","  adding: best_rf_model/stages/2_VectorAssembler_89c2eb4ae44a/metadata/ (stored 0%)\n","  adding: best_rf_model/stages/2_VectorAssembler_89c2eb4ae44a/metadata/.part-00000.crc (stored 0%)\n","  adding: best_rf_model/stages/2_VectorAssembler_89c2eb4ae44a/metadata/part-00000 (deflated 39%)\n","  adding: best_rf_model/stages/2_VectorAssembler_89c2eb4ae44a/metadata/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/2_VectorAssembler_89c2eb4ae44a/metadata/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/ (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/data/ (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/data/.part-00000-0b332001-3a09-422a-907a-8ee5e3bd61eb-c000.snappy.parquet.crc (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/data/part-00000-0b332001-3a09-422a-907a-8ee5e3bd61eb-c000.snappy.parquet (deflated 34%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/data/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/data/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/metadata/ (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/metadata/.part-00000.crc (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/metadata/part-00000 (deflated 36%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/metadata/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/1_StringIndexer_0b0a2d43843d/metadata/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/ (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/data/ (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/data/part-00000-166e5e44-9a19-4e0a-9b6e-d0662ff9aeb8-c000.snappy.parquet (deflated 30%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/data/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/data/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/data/.part-00000-166e5e44-9a19-4e0a-9b6e-d0662ff9aeb8-c000.snappy.parquet.crc (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/metadata/ (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/metadata/.part-00000.crc (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/metadata/part-00000 (deflated 35%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/metadata/_SUCCESS (stored 0%)\n","  adding: best_rf_model/stages/0_StringIndexer_33de8b91d006/metadata/._SUCCESS.crc (stored 0%)\n","  adding: best_rf_model/metadata/ (stored 0%)\n","  adding: best_rf_model/metadata/.part-00000.crc (stored 0%)\n","  adding: best_rf_model/metadata/part-00000 (deflated 27%)\n","  adding: best_rf_model/metadata/_SUCCESS (stored 0%)\n","  adding: best_rf_model/metadata/._SUCCESS.crc (stored 0%)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_3b85f7ed-f7b9-4ddf-a368-4625db8771b3\", \"best_rf_model.zip\", 3057258)"]},"metadata":{}}]},{"cell_type":"code","source":["data = spark.read.csv(csv_path, header=True, inferSchema=True)\n"],"metadata":{"id":"lcaKPL5DKXaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, when\n","\n","# ƒê·∫∑c tr∆∞ng m·ªü r·ªông\n","data = data.withColumn(\"age_of_house\", 2025 - col(\"yr_built\"))\n","data = data.withColumn(\"was_renovated\", when(col(\"yr_renovated\") > 0, 1).otherwise(0))\n","data = data.withColumn(\"total_area\", col(\"sqft_living\") + col(\"sqft_basement\"))\n","\n","# √âp ki·ªÉu cho c√°c c·ªôt s·ªë\n","num_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","            'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built',\n","            'yr_renovated', 'age_of_house', 'was_renovated', 'total_area', 'price']\n","\n","for c in num_cols:\n","    data = data.withColumn(c, col(c).cast(\"double\"))\n","\n","data = data.dropna()\n"],"metadata":{"id":"Kq-0pO4hKjjB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n","\n","indexers = [\n","    StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\")\n","    for c in ['city', 'statezip']\n","]\n"],"metadata":{"id":"VGxgxDRgKnRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml import Pipeline\n","\n","feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","                'view', 'condition', 'sqft_above', 'sqft_basement', 'age_of_house',\n","                'was_renovated', 'total_area', 'city_idx', 'statezip_idx']\n","\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"price\",\n","    numTrees=300,\n","    maxDepth=20,\n","    maxBins=256,\n","    seed=42\n",")\n","\n","pipeline = Pipeline(stages=indexers + [assembler, rf])\n"],"metadata":{"id":"T-bTO9gzKpLN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","rf_model = pipeline.fit(train_data)\n","predictions = rf_model.transform(test_data)\n"],"metadata":{"id":"ZjyVUcZCKq1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","print(f\"‚úÖ Retrained RF RMSE: {rmse:.2f}\")\n","print(f\"‚úÖ Retrained RF MAE: {mae:.2f}\")\n","print(f\"‚úÖ Retrained RF R¬≤: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TsntcX_KwWe","executionInfo":{"status":"ok","timestamp":1750521516463,"user_tz":-420,"elapsed":4803,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"e3087143-4eea-452e-dc30-6dc62f057117"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Retrained RF RMSE: 1007454.93\n","‚úÖ Retrained RF MAE: 153601.33\n","‚úÖ Retrained RF R¬≤: 0.0763\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import col, when\n","from pyspark.ml.feature import StringIndexer, VectorAssembler\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# ƒê·ªçc d·ªØ li·ªáu g·ªëc\n","data = spark.read.csv(csv_path, header=True, inferSchema=True)\n","\n","# T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi\n","data = data.withColumn(\"age_of_house\", 2025 - col(\"yr_built\"))\n","data = data.withColumn(\"was_renovated\", when(col(\"yr_renovated\") > 0, 1).otherwise(0))\n","data = data.withColumn(\"total_area\", col(\"sqft_living\") + col(\"sqft_basement\"))\n","\n","# √âp ki·ªÉu s·ªë\n","num_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","            'view', 'condition', 'sqft_above', 'sqft_basement', 'yr_built',\n","            'yr_renovated', 'age_of_house', 'was_renovated', 'total_area', 'price']\n","\n","for c in num_cols:\n","    data = data.withColumn(c, col(c).cast(\"double\"))\n","\n","# X·ª≠ l√Ω thi·∫øu\n","data = data.dropna()\n","\n","# M√£ h√≥a c√°c bi·∫øn ph√¢n lo·∫°i\n","indexers = [\n","    StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\")\n","    for c in ['city', 'statezip']\n","]\n","\n","# G·ªôp c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o\n","feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n","                'view', 'condition', 'sqft_above', 'sqft_basement', 'age_of_house',\n","                'was_renovated', 'total_area', 'city_idx', 'statezip_idx']\n","\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","# T·∫°o m√¥ h√¨nh Random Forest t·ªët nh·∫•t\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\", labelCol=\"price\",\n","    numTrees=300, maxDepth=20, maxBins=256, seed=42\n",")\n","\n","# Pipeline t·ªïng h·ª£p\n","pipeline = Pipeline(stages=indexers + [assembler, rf])\n","\n","# Chia t·∫≠p train/test\n","train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Hu·∫•n luy·ªán m√¥ h√¨nh\n","rf_model = pipeline.fit(train_data)\n","\n","# D·ª± ƒëo√°n\n","predictions = rf_model.transform(test_data)\n","\n","# ƒê√°nh gi√° m√¥ h√¨nh\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n","rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n","mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n","r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n","\n","print(f\"‚úÖ RF (t·ªët nh·∫•t) RMSE: {rmse:.2f}\")\n","print(f\"‚úÖ RF (t·ªët nh·∫•t) MAE: {mae:.2f}\")\n","print(f\"‚úÖ RF (t·ªët nh·∫•t) R¬≤: {r2:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BEexQhOTLRrj","executionInfo":{"status":"error","timestamp":1750522240802,"user_tz":-420,"elapsed":592945,"user":{"displayName":"H√πng Th·∫Øng Nguy·ªÖn L√™","userId":"04142678779741849560"}},"outputId":"8c59ba78-5959-4f37-c380-5d2158362960"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:Exception while sending command.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 516, in send_command\n","    raise Py4JNetworkError(\"Answer from Java side is empty\")\n","py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n","    response = connection.send_command(command)\n","               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 539, in send_command\n","    raise Py4JNetworkError(\n","py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o45731.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6072.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6072.0 (TID 6052) (357c9aacbe54 executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-80-2769442474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Hu·∫•n luy·ªán m√¥ h√¨nh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# D·ª± ƒëo√°n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45731.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6072.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6072.0 (TID 6052) (357c9aacbe54 executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.$anonfun$train$1(RandomForestRegressor.scala:158)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:136)\n\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"HousePricePrediction\") \\\n","    .config(\"spark.driver.memory\", \"4g\") \\\n","    .config(\"spark.executor.memory\", \"4g\") \\\n","    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n","    .getOrCreate()\n"],"metadata":{"id":"CtyFuubxTkPc"},"execution_count":null,"outputs":[]}]}